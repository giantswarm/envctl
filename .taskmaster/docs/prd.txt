# PRD: The `envctl` Generic Orchestration Platform

<context>
This document outlines the final architecture for `envctl`, transforming it from a Kubernetes-centric tool into a truly generic orchestration platform. The core principle is the complete removal of hardcoded concepts like "clusters" and "port-forwards" from the `envctl` core.

The key enabling mechanism for this decoupling is the introduction of **`ServiceClass`** definitions, which act as blueprints for provisioning any required prerequisite for a managed service.
</context>

<vision>
# 1. Product Vision & User Value

## 1.1. Product Vision
`envctl`'s vision is to be the ultimate command-line interface for platform engineers and developers who interact with complex, multi-faceted environments. It will abstract away the low-level details of disparate tools (Kubernetes, cloud providers, CI/CD systems, internal APIs) and present them as a unified, cohesive, and automatable environment.

The long-term goal is for `envctl` to become an indispensable "mission control" for any developer's daily workflow, radically reducing cognitive load and accelerating productivity by making complex operations simple and repeatable.

## 1.2. User Value & Personas

`envctl` delivers distinct value to two key user personas:

*   **For the Platform Engineer:** `envctl` provides a standardized, extensible framework to build powerful, reusable tooling for their developer communities. By defining custom `ServiceClass` and `Capability` definitions, they can codify best practices, automate complex environment setup, and provide a consistent, high-quality developer experience at scale.

*   **For the Application Developer:** `envctl` drastically simplifies daily workflows. Instead of juggling multiple CLIs, configs, and authentication tokens, developers use one tool with a consistent API to connect to clusters, forward ports, inspect services, and run operational workflows. This means less time spent on environmental boilerplate and more time spent on writing code.

## 1.3. Goals of this Architectural Refactoring

This PRD describes a significant architectural refactoring. The primary goals of this work are:

1.  **Enable the Vision:** To establish a flexible, scalable, and maintainable core architecture that allows `envctl` to evolve into the generic orchestration platform described above.
2.  **Decouple Core from Domain:** To completely separate the `envctl` orchestration logic from any specific domain like Kubernetes, allowing it to manage any type of service or resource.
3.  **Drive Extensibility:** To allow new functionalities (e.g., connecting to a new type of database, managing a new cloud resource) to be added simply by creating new `ServiceClass` and `Capability` definitions, without changing the core `envctl` binary.
4.  **Improve Code Clarity:** To create a clean and logical codebase where each package has a single, well-defined responsibility, making it easier for new contributors to understand and extend the system.

</vision>
<PRD>
# 2. Core Architecture: Orchestrator + ServiceClass + Capabilities

## 2.1. The Core Orchestrator
The `Orchestrator` is the master lifecycle controller.
- Its only responsibility is to manage a list of running `Service` instances.
- All hardcoded dependency logic related to clusters and port-forwards is **removed**.
- Instead, a `Service` definition in `envctl.yaml` can now include a **`requires`** list. Before starting the service, the orchestrator ensures all `ServiceClass`es in this list are provisioned and running.
- The orchestrator exposes its service management actions through clearly namespaced MCP tools (using the `core_*` prefix).

## 2.2. The `ServiceClass` Layer
This is the new foundation for dynamic prerequisites.
- A **`ServiceClass`** is a reusable, versioned blueprint for a service, defined in a YAML file (e.g., `sc_k8s_connection.yaml`). It describes *how* to manage a service's lifecycle (`create`, `delete`, `health_check`) using `envctl` tools.
- A **`ServiceInstance`** is a live, running instance of a `ServiceClass`, managed by the `Orchestrator`.
- A new `ServiceClassManager` is responsible for loading and providing all available `ServiceClass` definitions.

## 2.3. The Decoupled `capability` Layer
- The `capability` package is purified to its original intent: providing domain-specific API abstractions (e.g., `api_cluster_connect`). It translates a simple, user-friendly API call into one or more underlying tool calls. It knows nothing about service lifecycles.

## 2.4. The New Decoupled Flow
A Prometheus MCP server that needs a Kubernetes connection and a port-forward is now defined like this:
```yaml
mcpServers:
  - name: prometheus
    type: container
    image: "giantswarm/mcp-prometheus"
    # The orchestrator will provision these prerequisites BEFORE starting the container.
    requires:
      # Each key becomes the label for the ServiceInstance.
      - prometheus_db_conn:
          # This is the name of the blueprint to use.
          serviceClassName: k8s-connection
          # These are the parameters for this specific instance.
          parameters:
            clusterName: "my-obs-cluster"
      - prometheus_port_fwd:
          serviceClassName: port-forward
          parameters:
            # We can reference outputs from other required instances.
            clusterURI: "{{ dependencies.prometheus_db_conn.outputs.uri }}"
            namespace: "monitoring"
            service: "prometheus-k8s"
            port: "9090"
    # The outputs from prerequisite instances can be injected into the service.
    env:
      PROMETHEUS_ENDPOINT: "{{ dependencies.prometheus_port_fwd.outputs.endpoint }}"
```

# 3. Tool Naming Convention

To provide clarity and avoid conflicts, envctl will use the following tool naming prefixes:

**Management APIs (Core envctl functions):**
- `core_service_class_*` - ServiceClass blueprint management (e.g., `core_service_class_list`, `core_service_class_get`)
- `core_service_*` - Running ServiceInstance management (e.g., `core_service_list`, `core_service_get`, `core_service_stop`)
- `core_workflow_*` - Workflow management API
- `core_capability_*` - Capability management API
- `core_config_*` - Configuration management API

**Execution Tools (User-facing operations):**
- `workflow_*` - Actual workflow executions
- `api_*` - Actual capability operations
- Configurable prefix (default `x_`) - For external MCP server tools to avoid conflicts

# 4. Development Roadmap

## Phase 1: Generic Orchestrator & Initial Cleanup
- **Goal**: Decouple the orchestrator from hardcoded concepts.
- **Tasks**:
    - **Remove** `internal/kube`, `internal/portforwarding`, and `ClusterDefinition`/`PortForwardDefinition` from `internal/config`.
    - Modify `envctl connect` to remove its cluster-specific parameters.
    - **Remove** the unused `capability` service.

## Phase 2: Implement the `ServiceClass` Architecture
- **Goal**: Implement the core `ServiceClass` architecture as the foundation for dynamic service management. This is a refactoring of misplaced logic from the `capability` package into the correct locations.
- **Tasks**:
    - **Implement `serviceclass` Package**: Create `internal/serviceclass` and the `ServiceClassManager` to load, parse, and validate `ServiceClass` YAML definitions. **Move** definition-loading logic here from the `capability` package.
    - **Implement Generic `ServiceInstance`**: Create a generic `ServiceInstance` in `internal/services`. This object will implement the `services.Service` interface. Its `Start`/`Stop`/`CheckHealth` methods will dynamically execute tools based on the `ServiceClass` definition it is configured with. **Move** lifecycle-execution logic here from the `capability` package.
    - **Refactor Orchestrator**: Update the `Orchestrator` to read the `requires` block from a service config, use the `ServiceClassManager` to get definitions, and instantiate/register `ServiceInstance` objects. **Move** instance-tracking logic here from the `capability` package.
    - **Update API Layer**: Refactor `internal/api/handlers.go` to add the `ServiceClassManagerHandler` and refine the `OrchestratorHandler`.
    - **Purify `capability` Package**: **Delete** `service_orchestrator.go` and `service_registry.go` from `internal/capability`, leaving it pure.

## Phase 3: Build Default `ServiceClass` and `Capability` Libraries
- **Goal**: Re-implement the functionality of the old `internal/kube` and `internal/portforwarding` packages using the new, flexible architecture.
- **Tasks**:
    - **Create Default `ServiceClass` Definitions**:
      - `sc_k8s_connection.yaml`: A blueprint for a Kubernetes connection `ServiceInstance`.
      - `sc_port_forward.yaml`: A blueprint for a port-forward `ServiceInstance`.
    - **Create Default `Capability` Definitions**:
      - `cluster.capability.yaml`: Defines user-friendly `api_cluster_*` tools that wrap external `kubernetes-mcp` tools.
      - `portforward.capability.yaml`: Defines `api_portforward_*` tools.
    - **Integrate Capabilities with ServiceClasses**: An `api_portforward_create` call, for example, would not only call the backend tool but could also be configured to create a long-running `port-forward` `ServiceInstance` to monitor the connection.

## Phase 4: TUI as an MCP Client
- **Goal**: Decouple the TUI from the `envctl` core.
- **Tasks**:
    - The TUI becomes a standalone MCP client that connects to the aggregated MCP server provided by `envctl`.
    - Update the TUI to use the new execution tools (`api_*`, `workflow_*`).
    - Provide default `config.yaml` examples showing how to compose services with `ServiceClass` requirements.
</PRD>
