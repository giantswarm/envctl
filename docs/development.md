# Developing envctl

This document provides guidelines and information for developers contributing to the envctl project.

## Prerequisites

*   Go 1.21+
*   Make
*   Docker (for `act` testing)
*   [act](https://github.com/nektos/act#installation) (optional, for local workflow testing)
*   Python 3+ & pip (for `yamllint`)
*   `yamllint` (`pip install yamllint`)
*   `golangci-lint` (can be installed via the CI workflow or locally: [Install Instructions](https://golangci-lint.run/usage/install/#local-installation))

## Development Setup

1. Clone the repository:
   ```zsh
   git clone https://github.com/giantswarm/envctl.git
   cd envctl
   ```

2. Install Go dependencies:
   ```zsh
   go mod download
   ```

3. Build the binary:
   ```zsh
   make build
   ```

4. Install locally for testing:
   ```zsh
   make install
   ```

## Linting and Testing Strategy

*   **Go Linting:** Handled by CircleCI (not run in this repository's GitHub Actions workflows).
*   **YAML Linting:** Performed using `yamllint`. Run locally using `make lint-yaml` or `make check` (which is also run in the CI workflow).
*   **Go Unit Tests:** Not currently run via Make targets or the GitHub Actions workflows in this repository. They should be run manually (`go test ./...`) or handled by a separate CI/CD process if configured (e.g., CircleCI).
*   **Release Dry-Run:** The GoReleaser configuration and build process can be tested using `make release-dry-run`. This is also run automatically in the CI workflow for pull requests.

## Automated Release Process

The envctl project uses GitHub Actions to automatically create releases when pull requests are merged to the main branch. Here's how it works:

1. When a PR is merged to `main`, the `.github/workflows/auto-release.yaml` GitHub Action workflow is triggered.
2. The workflow:
   * Gets the latest version tag (defaulting to v0.0.0).
   * Creates a new version by incrementing the patch number (e.g., v0.1.0 -> v0.1.1).
   * Creates and pushes a new git tag corresponding to the new version.
   * Uses GoReleaser (`goreleaser release --clean`) to build binaries, create archives, generate checksums, and create a GitHub Release.
   * The release notes (changelog) are automatically generated by GoReleaser using GitHub's API based on merged PRs since the last tag (`changelog.use: github-native`).

## Testing the Release Process

You can test parts of the release process locally using the following commands:

```zsh
# Test the GoReleaser build/archive/checksum steps without publishing
make release-dry-run

# Create a full release locally (builds, archives, checksums, GitHub release)
# Requires a GITHUB_TOKEN environment variable with repo scope.
# Note: This creates a real release and should generally only be used by maintainers for specific testing.
make release-local
```

### Testing the GitHub Actions Workflows Locally with `act`

You can use [`act`](https://github.com/nektos/act#installation) to simulate the GitHub Actions workflows locally. This requires Docker. Use the following `make` targets:

1. **Test the CI Workflow (Pull Request Event):**
   ```zsh
   # This simulates the checks run on a PR (yamllint, goreleaser dry-run)
   make test-ci-pr
   ```

2. **Test the CI Workflow (Push Event):**
   ```zsh
   # This simulates the checks run on a push to main (yamllint)
   make test-ci-push
   ```

3. **Test the Auto-Release Workflow:**
   ```zsh
   # This simulates the full release process
   make test-auto-release
   ```
   * **Note:** Requires `merged_pr_event.json` in the project root. You can create one based on the [GitHub pull_request event docs](https://docs.github.com/en/actions/using-workflows/events-that-trigger-workflows#pull_request) (ensure `.pull_request.merged` is `true`).
   * **Note:** The step involving `git push` for the tag might fail locally if the tag already exists or due to credential issues. You also won't see the GitHub Release creation locally.

## Contributing

1. Create a new branch for your changes:
   ```zsh
   git checkout -b feature/your-feature-name
   ```

2. Make your changes and commit them with meaningful commit messages.

3. Ensure your changes pass local checks (e.g., `make check`). Run Go linters and unit tests separately if they are not covered by other CI.

4. Push your branch and create a pull request targeting the `main` branch.

5. The CI workflow (`ci.yaml`) will run YAML linting checks (`make check`) and a GoReleaser dry-run on your PR.

6. Ensure your PR includes any necessary updates, but **do not manually update `CHANGELOG.md`**. The changelog will be generated automatically during the release.

7. Once your PR is approved and merged, the auto-release workflow will run and create a new release with generated notes.

## Version Management

The version number follows Semantic Versioning (MAJOR.MINOR.PATCH).

* The automated release process currently **only increments the PATCH version** for each merged PR.
* For **MINOR** or **MAJOR** version bumps, manual intervention is currently required after merging the relevant feature/breaking change PRs:
  1. Create and push the desired tag manually (e.g., `git tag v1.0.0`, `git push origin v1.0.0`).
  2. Manually trigger the `auto-release.yaml` workflow via the GitHub Actions UI, selecting the `main` branch and providing the manually created tag.
  *Alternatively, adjust the `Determine Next Version` step in `auto-release.yaml` temporarily before merging the PR that should trigger the bump, or create a separate release PR.*

## TUI Implementation Details

For detailed information about the TUI (Terminal User Interface) architecture, components, and how it interacts with core logic, please refer to the [TUI Implementation Notes](./tui-implementation.md).

## Unified Service Architecture

envctl uses a unified service architecture to manage all types of services (K8s connections, port forwards, and MCP servers) consistently. This architecture is built on two main components:

- **Service Manager**: Handles basic lifecycle operations for all services
- **Orchestrator**: Manages dependencies, health monitoring, and coordinated operations

For a comprehensive understanding of how services are managed, including:
- How K8s connections, port forwards, and MCP servers are treated as services
- The dependency graph and cascade operations
- Health monitoring and automatic recovery
- State management and reporting

Please refer to the [Architecture Overview](architecture.md) and the sections below on the Orchestrator and Service Manager.

## Orchestrator and Service Manager Architecture

The envctl application uses a sophisticated architecture to manage service dependencies, health monitoring, and lifecycle management. This is accomplished through two primary components: the **Orchestrator** and the **Service Manager**.

### Service Manager (`internal/managers`)

The Service Manager is responsible for the basic lifecycle management of services (port forwards, MCP servers, and K8s connections). It provides a simple, dependency-agnostic interface for starting and stopping services.

**Key Responsibilities:**
- Starting batches of services
- Stopping individual services or all services
- Tracking active services
- Reporting service state changes through a reporter interface
- Managing the mapping between service labels and their configurations
- Managing K8s connection services (when initialized with a KubeManager)

**Core Interface (`ServiceManagerAPI`):**
```go
type ServiceManagerAPI interface {
    StartServices(configs []ManagedServiceConfig, wg *sync.WaitGroup) (map[string]chan struct{}, []error)
    StopService(label string) error
    StopAllServices()
    SetReporter(reporter reporting.ServiceReporter)
    GetServiceConfig(label string) (ManagedServiceConfig, bool)
    IsServiceActive(label string) bool
    GetActiveServices() []string
}
```

**Design Principles:**
1. **Simplicity**: The Service Manager focuses only on basic start/stop operations
2. **No Dependency Logic**: It doesn't understand or manage dependencies between services
3. **State Tracking**: Maintains a simple map of active services and their stop channels
4. **Reporter Pattern**: Uses a reporter interface to notify about service state changes
5. **K8s Integration**: When created with a KubeManager, it can manage K8s connection services

### Orchestrator (`internal/orchestrator`)

The Orchestrator is the higher-level component that manages the overall application state, including dependency management, health monitoring, and coordinated service lifecycle operations.

**Key Responsibilities:**
- Building and managing the dependency graph
- Monitoring Kubernetes connection health
- Managing cascade stops when dependencies fail
- Ensuring services start in the correct dependency order
- Handling service restarts with dependency awareness
- Tracking why services were stopped (manual vs dependency failure)

**Core Features:**

#### 1. Dependency Management
The Orchestrator builds a dependency graph (`internal/dependency`) that models relationships between:
- Kubernetes connections (MC and WC)
- Port forwards (which depend on K8s connections)
- MCP servers (which may depend on port forwards)

Example dependency chain:
```
K8s MC Connection → prometheus-mc (port forward) → prometheus (MCP)
K8s WC Connection → grafana-wc (port forward) → grafana (MCP)
```

**Dependency Graph Structure:**
- **Nodes**: Each service (K8s connection, port forward, or MCP server) is a node in the graph
- **Edges**: Dependencies are directed edges from dependent to dependency
- **Node Types**: `KindK8sConnection`, `KindPortForward`, `KindMCP`
- **Node IDs**: Prefixed strings like `k8s:context-name`, `pf:service-name`, `mcp:server-name`

**Dependency Rules:**
1. **Port Forwards** depend on their target K8s connection (specified by `KubeContextTarget`)
2. **MCP Servers** can depend on:
   - Port forwards (specified in `RequiresPortForwards` array)
   - K8s connections (e.g., the "kubernetes" MCP depends on MC connection)
3. **K8s Connections** have no dependencies (they are the foundation)

**Starting Services with Dependencies:**
When starting services, the Orchestrator:
1. Groups services by dependency levels using topological sort
2. Starts services level by level (level 0 first, then 1, etc.)
3. Waits for services in each level to become ready before starting the next level
4. Only starts services whose K8s dependencies are healthy

Example startup order:
- Level 0: K8s connections (no dependencies)
- Level 1: Port forwards (depend on K8s connections)
- Level 2: MCP servers (depend on port forwards)

**Restarting Services with Dependencies:**
When restarting a service, the Orchestrator now ensures all its dependencies are also running:
1. Checks if the service's dependencies are active
2. If any dependency is not active, it includes it for restart
3. Starts all services (the requested one plus its dependencies) in dependency order
4. This ensures a service always has its requirements satisfied

For example, restarting the "grafana" MCP server will also restart the "grafana-mc" port forward if it's not running.

#### 2. Health Monitoring
The Orchestrator continuously monitors Kubernetes connection health:
- Performs periodic health checks (default: 15 seconds)
- Tracks state changes (healthy → unhealthy → healthy)
- Only acts on state changes, not every health check
- Reports health status through the reporter interface

#### 3. Cascade Stop Logic
When a dependency becomes unhealthy or is stopped:
1. The Orchestrator identifies all transitive dependents using the dependency graph
2. It stops dependent services in reverse dependency order
3. It tracks that these services were stopped due to dependency failure (`StopReasonDependency`)
4. Example: If K8s WC connection fails → stop dependent port forwards → stop MCPs depending on those port forwards

**Stop Reasons:**
- `StopReasonManual`: User explicitly stopped the service (won't auto-restart)
- `StopReasonDependency`: Service stopped due to dependency failure (will auto-restart when dependency recovers)

#### 4. Automatic Recovery
When a failed dependency recovers:
1. The Orchestrator identifies services that were stopped due to that dependency
2. It restarts only those services (not manually stopped ones)
3. Services are started in proper dependency order
4. The restart is triggered automatically without user intervention

#### 5. Stop Reason Tracking
The Orchestrator maintains a `stopReasons` map to differentiate between:
- `StopReasonManual`: User explicitly stopped the service
- `StopReasonDependency`: Service stopped due to dependency failure

This ensures that only dependency-stopped services are restarted during recovery.

### Integration and Workflow

#### Startup Flow:
1. **Orchestrator Initialization**: Creates dependency graph based on configuration
2. **Health Check**: Performs initial K8s health checks
3. **Service Start**: Starts only services whose dependencies are healthy
4. **Dependency Ordering**: Uses topological sort to start services in correct order
5. **Monitoring**: Begins periodic health monitoring

#### Health State Change Flow:
1. **Detection**: Health check detects K8s connection state change
2. **Analysis**: Orchestrator analyzes dependency graph for affected services
3. **Action**: 
   - If unhealthy: Stop dependent services with cascade logic
   - If recovered: Restart dependency-stopped services
4. **Reporting**: Updates reported through the reporter interface

#### Service Operations:
- **Start**: Orchestrator ensures dependencies are met before starting
- **Stop**: Orchestrator performs cascade stop for dependents
- **Restart**: Orchestrator marks for restart and manages the lifecycle

### Usage in TUI and Non-TUI Modes

Both TUI and non-TUI modes use the same Orchestrator/Service Manager architecture:

**TUI Mode (`internal/tui`):**
- The TUI controller creates and manages the Orchestrator
- Service operations (stop/restart) go through the Orchestrator
- State updates are received via the reporter interface and converted to TUI messages
- The TUI model tracks service states for display

**Non-TUI Mode (`cmd/connect.go`):**
- Creates Orchestrator with console reporter
- Starts services through the Orchestrator
- Handles graceful shutdown on interrupt
- Displays health and service status to console

### Testing Considerations

The architecture supports comprehensive testing through:
1. **Mock Interfaces**: Both Service Manager and Kube Manager can be mocked
2. **Dependency Injection**: Components are injected, making testing easier
3. **State Verification**: Tests can verify stop reasons and service states
4. **Timing Control**: Health check intervals can be adjusted for faster tests

Example test pattern:
```go
// Create mocks
serviceMgr := newMockServiceManager()
reporter := &mockReporter{}

// Configure test scenario
cfg := Config{
    MCName: "test-mc",
    WCName: "test-wc",
    PortForwards: []config.PortForwardDefinition{...},
    MCPServers: []config.MCPServerDefinition{...},
}

// Create orchestrator
orch := New(serviceMgr, reporter, cfg)

// Simulate K8s health state changes
orch.GetK8sStateManager().UpdateConnectionState("test-mc", state.ConnectionState{
    IsHealthy: true,
    NodeCount: 5,
})

// Run test scenario
err := orch.Start(ctx)

// Verify cascade stop behavior
// ... test assertions ...
```

### Best Practices for Extending

When adding new features that involve service management:

1. **Use the Orchestrator**: Don't bypass it for service operations
2. **Update Dependencies**: If adding new service types, update the dependency graph builder
3. **Maintain Separation**: Keep basic operations in Service Manager, complex logic in Orchestrator
4. **Test Thoroughly**: Include tests for dependency scenarios and edge cases
5. **Consider Health**: If the service depends on external resources, integrate with health monitoring

## Dependency Graph Implementation

The dependency graph is implemented in `internal/dependency/graph.go` and provides a simple but effective way to model service relationships.

### Core Types

```go
// NodeID is the unique identifier for a node
type NodeID string

// Node represents a service with its dependencies
type Node struct {
    ID           NodeID
    FriendlyName string
    Kind         NodeKind
    DependsOn    []NodeID  // What this node depends on
    State        NodeState // Current lifecycle state
}

// Graph manages the collection of nodes
type Graph struct {
    nodes map[NodeID]*Node
}
```

### Key Methods

1. **`AddNode(n Node)`**: Adds or replaces a node in the graph
2. **`Get(id NodeID) *Node`**: Retrieves a node by ID
3. **`Dependencies(id NodeID) []NodeID`**: Returns what a node depends ON
4. **`Dependents(id NodeID) []NodeID`**: Returns what depends on this node

### Usage Example

```go
// Building the graph (in orchestrator.buildDependencyGraph)
g := dependency.New()

// Add K8s connection node
g.AddNode(dependency.Node{
    ID:           "k8s:my-cluster",
    FriendlyName: "K8s Connection",
    Kind:         dependency.KindK8sConnection,
    DependsOn:    nil, // No dependencies
})

// Add port forward that depends on K8s
g.AddNode(dependency.Node{
    ID:           "pf:prometheus",
    FriendlyName: "Prometheus Port Forward",
    Kind:         dependency.KindPortForward,
    DependsOn:    []dependency.NodeID{"k8s:my-cluster"},
})

// Add MCP that depends on port forward
g.AddNode(dependency.Node{
    ID:           "mcp:prometheus",
    FriendlyName: "Prometheus MCP Server",
    Kind:         dependency.KindMCP,
    DependsOn:    []dependency.NodeID{"pf:prometheus"},
})
```

### Finding Dependencies

The Orchestrator uses `findAllDependents` to find all transitive dependents:

```go
// findAllDependents finds all services that depend on the given node
func (o *Orchestrator) findAllDependents(nodeID string) []string {
    allDependents := make(map[string]bool)
    visited := make(map[string]bool)
    
    var findDependents func(currentID string)
    findDependents = func(currentID string) {
        if visited[currentID] {
            return
        }
        visited[currentID] = true
        
        // Get direct dependents
        directDependents := o.depGraph.Dependents(dependency.NodeID(currentID))
        
        for _, dep := range directDependents {
            depStr := string(dep)
            if !strings.HasPrefix(depStr, "k8s:") {
                allDependents[depStr] = true
            }
            // Recursively find dependents
            findDependents(depStr)
        }
    }
    
    findDependents(nodeID)
    return keys(allDependents)
}
```

This recursive approach ensures that stopping a K8s connection will cascade through all port forwards to all MCP servers that transitively depend on it.

## Service Health Monitoring

envctl includes comprehensive health monitoring for all managed services:

### Kubernetes Connection Health
- Monitors the health of Management Cluster (MC) and Workload Cluster (WC) connections
- Checks node readiness and API server availability every 15 seconds
- Automatically stops dependent services when a connection becomes unhealthy
- Restarts services when connections recover

### MCP Server Health Checks
- Verifies MCP servers are responsive by attempting to list their tools via JSON-RPC
- Uses the `tools/list` method over HTTP to validate the server is functioning
- Checks run every 30 seconds for all running MCP servers
- Health status is reflected in the TUI with appropriate indicators

### Port Forward Health Checks
- Validates port forwards by attempting TCP connections to the local port
- Ensures the tunnel is active and accepting connections
- Runs every 30 seconds for all active port forwards
- Failed health checks are reported but don't automatically stop the service

### Health Check Implementation
The health checking system is implemented in `internal/orchestrator/health_checker.go` with:
- `ServiceHealthChecker` interface for extensibility
- `MCPHealthChecker` for MCP server validation
- `PortForwardHealthChecker` for port forward validation
- Automatic health monitoring started by the orchestrator

### State Reconciliation
The TUI includes a `ReconcileState()` method that:
- Ensures the TUI model reflects the true state from the StateStore
- Runs on startup and can be triggered manually
- Prevents state drift between the TUI and the underlying services

## Dependency Management

## Recent Architectural Improvements

### K8s Connections as Services (Issue #46)
Previously, K8s connections were managed separately from other services. This has been refactored to treat K8s connections as first-class services:

**Benefits:**
- Unified service management across all service types
- Consistent health monitoring and state reporting
- Better integration with the dependency system
- Simplified orchestrator logic

**Implementation:**
- K8s connections are now managed by `K8sConnectionService` in `internal/managers/k8s_service.go`
- They report state changes through the same reporting system as other services
- Health checks are integrated into the service lifecycle
- The old `k8smanager` package has been removed

### Enhanced State Management (Issue #46)
The state management system has been significantly improved to address issues with state duplication and message ordering:

**Phase 1: Unified State Management**
- Added helper methods to TUI Model to use StateStore as single source of truth
- Implemented `GetServiceState()`, `GetServiceSnapshot()`, `ReconcileState()` methods
- TUI controller now queries StateStore instead of maintaining separate state

**Phase 2: Message Sequencing**
- Added sequence numbers to `ManagedServiceUpdate` messages
- Implemented `MessageBuffer` for handling out-of-order messages
- Ensures state updates are applied in the correct order

**Phase 3: Enhanced Correlation Tracking**
- Added `CascadeInfo` type for tracking cascade relationships
- Added `StateTransition` type for tracking state changes
- StateStore automatically records transitions and cascades
- Correlation IDs link related operations across services

**Phase 4: Improved Error Handling**
- Added retry logic for dropped critical updates
- Implemented backpressure notifications for users
- Enhanced TUIReporter with metrics and retry handling

### Dependency-Aware Service Restart
The orchestrator now properly handles service restarts with dependency awareness:

**When Restarting a Service:**
1. The orchestrator checks if all dependencies are running
2. If any dependency is stopped, it's included in the restart operation
3. All services are started in proper dependency order
4. This ensures services always have their requirements satisfied

**Example:**
Restarting the "grafana" MCP server will automatically restart the "mc-grafana" port forward if it's not running.

### Stop Reason Tracking
Services now track why they were stopped:

**Stop Reasons:**
- `StopReasonManual`: User explicitly stopped the service
- `StopReasonDependency`: Service stopped due to dependency failure

**Behavior:**
- Manually stopped services won't auto-restart
- Dependency-stopped services auto-restart when dependencies recover
- The monitoring goroutine respects these stop reasons

### Improved Restart Delay
When restarting services (especially port forwards), a 1-second delay is added to ensure:
- Ports are properly released by the OS
- Prevents "address already in use" errors
- Allows graceful cleanup of resources

## Testing Strategy

### Unit Tests
The codebase includes comprehensive unit tests for:
- Orchestrator dependency management
- Service lifecycle operations
- State management
- Health checking logic

### Mock Interfaces
Key interfaces are mocked for testing:
- `ServiceManagerAPI` for service operations
- `KubeManager` for Kubernetes operations
- `ServiceReporter` for state reporting

### Test Patterns
Common test patterns include:
- Simulating health state changes
- Verifying cascade stop behavior
- Testing restart with dependencies
- Checking stop reason tracking

Example test setup:
```go
// Create mocks
serviceMgr := newMockServiceManager()
reporter := &mockReporter{}

// Configure test scenario
cfg := Config{
    MCName: "test-mc",
    WCName: "test-wc",
    PortForwards: []config.PortForwardDefinition{...},
    MCPServers: []config.MCPServerDefinition{...},
}

// Create orchestrator
orch := New(serviceMgr, reporter, cfg)

// Simulate K8s health state changes
orch.GetK8sStateManager().UpdateConnectionState("test-mc", state.ConnectionState{
    IsHealthy: true,
    NodeCount: 5,
})

// Run test scenario
err := orch.Start(ctx)

// Verify cascade stop behavior
// ... test assertions ...
```

## Code Organization

### Package Structure
```
internal/
├── orchestrator/       # High-level coordination and dependency management
│   ├── orchestrator.go # Main orchestrator logic
│   ├── health_checker.go # Health monitoring implementation
│   └── orchestrator_test.go # Comprehensive tests
├── managers/          # Service lifecycle management
│   ├── manager.go     # Service manager implementation
│   ├── k8s_service.go # K8s connection service
│   └── types.go       # Common types
├── dependency/        # Dependency graph implementation
│   └── graph.go       # Graph data structure and algorithms
├── state/            # State management
│   └── k8s_state.go  # K8s connection state tracking
├── reporting/        # State reporting and updates
│   ├── types.go      # Message types and interfaces
│   ├── store.go      # State store implementation
│   └── buffer.go     # Message buffering
└── tui/              # Terminal UI implementation
    ├── model/        # TUI state and business logic
    ├── view/         # UI rendering
    └── controller/   # User input handling
```

### Key Interfaces
1. **ServiceManagerAPI**: Core service lifecycle operations
2. **ServiceReporter**: State change notifications
3. **K8sStateManager**: K8s connection state tracking
4. **ServiceHealthChecker**: Health check abstraction

## Future Improvements

### Planned Enhancements
1. **Persistent State**: Save and restore service state between sessions
2. **Custom Health Checks**: Allow user-defined health check scripts
3. **Service Groups**: Start/stop related services as a unit
4. **Enhanced Recovery**: Exponential backoff and circuit breakers

### Technical Debt
1. **Configuration Validation**: Stronger validation of service configurations
2. **Error Aggregation**: Better collection and presentation of multi-service errors
3. **Performance Monitoring**: Metrics for service startup times and resource usage
