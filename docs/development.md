# Developing envctl

This document provides guidelines and information for developers contributing to the envctl project.

## Prerequisites

*   Go 1.21+
*   Make
*   Docker (for `act` testing)
*   [act](https://github.com/nektos/act#installation) (optional, for local workflow testing)
*   Python 3+ & pip (for `yamllint`)
*   `yamllint` (`pip install yamllint`)
*   `golangci-lint` (can be installed via the CI workflow or locally: [Install Instructions](https://golangci-lint.run/usage/install/#local-installation))

## Development Setup

1. Clone the repository:
   ```zsh
   git clone https://github.com/giantswarm/envctl.git
   cd envctl
   ```

2. Install Go dependencies:
   ```zsh
   go mod download
   ```

3. Build the binary:
   ```zsh
   make build
   ```

4. Install locally for testing:
   ```zsh
   make install
   ```

## Linting and Testing Strategy

*   **Go Linting:** Handled by CircleCI (not run in this repository's GitHub Actions workflows).
*   **YAML Linting:** Performed using `yamllint`. Run locally using `make lint-yaml` or `make check` (which is also run in the CI workflow).
*   **Go Unit Tests:** Not currently run via Make targets or the GitHub Actions workflows in this repository. They should be run manually (`go test ./...`) or handled by a separate CI/CD process if configured (e.g., CircleCI).
*   **Release Dry-Run:** The GoReleaser configuration and build process can be tested using `make release-dry-run`. This is also run automatically in the CI workflow for pull requests.

## Automated Release Process

The envctl project uses GitHub Actions to automatically create releases when pull requests are merged to the main branch. Here's how it works:

1. When a PR is merged to `main`, the `.github/workflows/auto-release.yaml` GitHub Action workflow is triggered.
2. The workflow:
   * Gets the latest version tag (defaulting to v0.0.0).
   * Creates a new version by incrementing the patch number (e.g., v0.1.0 -> v0.1.1).
   * Creates and pushes a new git tag corresponding to the new version.
   * Uses GoReleaser (`goreleaser release --clean`) to build binaries, create archives, generate checksums, and create a GitHub Release.
   * The release notes (changelog) are automatically generated by GoReleaser using GitHub's API based on merged PRs since the last tag (`changelog.use: github-native`).

## Testing the Release Process

You can test parts of the release process locally using the following commands:

```zsh
# Test the GoReleaser build/archive/checksum steps without publishing
make release-dry-run

# Create a full release locally (builds, archives, checksums, GitHub release)
# Requires a GITHUB_TOKEN environment variable with repo scope.
# Note: This creates a real release and should generally only be used by maintainers for specific testing.
make release-local
```

### Testing the GitHub Actions Workflows Locally with `act`

You can use [`act`](https://github.com/nektos/act#installation) to simulate the GitHub Actions workflows locally. This requires Docker. Use the following `make` targets:

1. **Test the CI Workflow (Pull Request Event):**
   ```zsh
   # This simulates the checks run on a PR (yamllint, goreleaser dry-run)
   make test-ci-pr
   ```

2. **Test the CI Workflow (Push Event):**
   ```zsh
   # This simulates the checks run on a push to main (yamllint)
   make test-ci-push
   ```

3. **Test the Auto-Release Workflow:**
   ```zsh
   # This simulates the full release process
   make test-auto-release
   ```
   * **Note:** Requires `merged_pr_event.json` in the project root. You can create one based on the [GitHub pull_request event docs](https://docs.github.com/en/actions/using-workflows/events-that-trigger-workflows#pull_request) (ensure `.pull_request.merged` is `true`).
   * **Note:** The step involving `git push` for the tag might fail locally if the tag already exists or due to credential issues. You also won't see the GitHub Release creation locally.

## Contributing

1. Create a new branch for your changes:
   ```zsh
   git checkout -b feature/your-feature-name
   ```

2. Make your changes and commit them with meaningful commit messages.

3. Ensure your changes pass local checks (e.g., `make check`). Run Go linters and unit tests separately if they are not covered by other CI.

4. Push your branch and create a pull request targeting the `main` branch.

5. The CI workflow (`ci.yaml`) will run YAML linting checks (`make check`) and a GoReleaser dry-run on your PR.

6. Ensure your PR includes any necessary updates, but **do not manually update `CHANGELOG.md`**. The changelog will be generated automatically during the release.

7. Once your PR is approved and merged, the auto-release workflow will run and create a new release with generated notes.

## Version Management

The version number follows Semantic Versioning (MAJOR.MINOR.PATCH).

* The automated release process currently **only increments the PATCH version** for each merged PR.
* For **MINOR** or **MAJOR** version bumps, manual intervention is currently required after merging the relevant feature/breaking change PRs:
  1. Create and push the desired tag manually (e.g., `git tag v1.0.0`, `git push origin v1.0.0`).
  2. Manually trigger the `auto-release.yaml` workflow via the GitHub Actions UI, selecting the `main` branch and providing the manually created tag.
  *Alternatively, adjust the `Determine Next Version` step in `auto-release.yaml` temporarily before merging the PR that should trigger the bump, or create a separate release PR.*

## TUI Implementation Details

For detailed information about the TUI (Terminal User Interface) architecture, components, and how it interacts with core logic, please refer to the [TUI Implementation Notes](./tui-implementation.md).

## Package Design for Shared Core Logic (Example: MCP Server Management)

When implementing features that need to operate in both TUI (Terminal User Interface) mode and non-TUI/CLI mode, or that manage external processes, a clean separation of concerns is crucial. The management of Management Cluster Proxy (MCP) servers serves as a good example of this approach in `envctl`.

**Core Principles:**

1.  **Dedicated Core Package:** Logic that is fundamental to the feature itself, independent of its presentation (TUI or CLI), should reside in its own internal package. For MCP servers, this is `internal/mcpserver`.
2.  **Agnostic Core:** This core package should be agnostic to its consumers. It should not contain TUI-specific code (e.g., `tea.Msg` types) or CLI-specific code (e.g., direct `fmt.Println` for primary output).
3.  **Static Configuration:** Default configurations for entities managed by the core package (like the list of predefined MCP servers) should reside within the core package, ideally as static data (`internal/mcpserver/config.go`). This makes the core package the single source of truth for these definitions.
4.  **Callback/Interface for Updates:** To communicate status, logs, or errors from managed processes, the core package should use generic callbacks or interfaces. For MCP servers, `mcpserver.McpUpdateFunc` (a function type taking `mcpserver.McpProcessUpdate`) is used. The core process manager (`mcpserver.StartAndManageIndividualMcpServer`) calls this function with updates.
5.  **Consumer Responsibility:** The consumers (TUI mode logic or non-TUI mode logic) are responsible for:
    *   Deciding *when* and *which* core functionalities to invoke (e.g., which MCP servers to start).
    *   Providing a mode-specific implementation of the callback (`McpUpdateFunc`). The TUI's implementation translates `McpProcessUpdate` into `tea.Msg`s for its event loop, while the non-TUI mode's implementation prints to the console.

**MCP Server Implementation Example (`internal/mcpserver`):**

*   **`types.go`:** Defines `PredefinedMcpServer` (structure for server config), `McpProcessUpdate` (generic update bundle), and `McpUpdateFunc` (callback signature).
*   **`config.go`:** Contains the static `PredefinedMcpServers` list.
*   **`process.go`:** Houses `StartAndManageIndividualMcpServer`. This function:
    *   Takes a `PredefinedMcpServer` configuration.
    *   Internally prepares the `exec.Cmd` to run `mcp-proxy` with the correct arguments for the specified underlying server.
    *   Manages the lifecycle of this process (start, stop signal, log streaming).
    *   Uses the provided `McpUpdateFunc` to report all events (initial "Running" status, logs, errors, final "Stopped" status).
    *   Returns a `stopChan` to allow the caller to signal termination, and any initial startup error.
*   **`startup.go`:** Contains `StartAllPredefinedMcpServers`. This is a helper function primarily for the non-TUI mode:
    *   It iterates `PredefinedMcpServers`.
    *   For each server, it calls `StartAndManageIndividualMcpServer`.
    *   It provides the `McpUpdateFunc` (which will be a console-printing one when called by `cmd/connect.go`).
    *   It returns a channel of `ManagedMcpServerInfo` (containing label, PID, stopChan, initial error) allowing the caller to get details for each server as it's being initiated.

**Consumption by TUI (`internal/tui/commands.go`):**

*   The TUI's `startMcpProxiesCmd` iterates `mcpserver.PredefinedMcpServers`.
*   For each server, it creates a `tea.Cmd`. This command, when executed:
    *   Defines an `McpUpdateFunc` that converts `mcpserver.McpProcessUpdate` into a TUI-specific `tea.Msg` (e.g., `tui.mcpServerStatusUpdateMsg`) and sends it to the TUI's main event channel.
    *   Calls `mcpserver.StartAndManageIndividualMcpServer` with the server's config and this TUI-specific update function.
    *   Returns an initial `tui.mcpServerSetupCompletedMsg` to the TUI model, including the `stopChan` and any immediate startup error.

**Consumption by Non-TUI (`cmd/connect.go`):**

*   The non-TUI mode in `cmd/connect.go` calls `mcpserver.StartAllPredefinedMcpServers`.
*   It provides an `McpUpdateFunc` that directly prints the `OutputLog` from `McpProcessUpdate` to `os.Stdout` or `os.Stderr`.
*   It ranges over the channel returned by `StartAllPredefinedMcpServers` to collect `stopChan`s for each successfully initiated server, which are used for graceful shutdown on Ctrl+C.

This layered approach ensures that `internal/mcpserver` is a reusable, independent engine, while the TUI and non-TUI modes adapt its usage to their specific operational and UI/output requirements.

## Orchestrator and Service Manager Architecture

The envctl application uses a sophisticated architecture to manage service dependencies, health monitoring, and lifecycle management. This is accomplished through two primary components: the **Orchestrator** and the **Service Manager**.

### Service Manager (`internal/managers`)

The Service Manager is responsible for the basic lifecycle management of services (port forwards and MCP servers). It provides a simple, dependency-agnostic interface for starting and stopping services.

**Key Responsibilities:**
- Starting batches of services
- Stopping individual services or all services
- Tracking active services
- Reporting service state changes through a reporter interface
- Managing the mapping between service labels and their configurations

**Core Interface (`ServiceManagerAPI`):**
```go
type ServiceManagerAPI interface {
    StartServices(configs []ManagedServiceConfig, wg *sync.WaitGroup) (map[string]chan struct{}, []error)
    StopService(label string) error
    StopAllServices()
    SetReporter(reporter reporting.ServiceReporter)
    GetServiceConfig(label string) (ManagedServiceConfig, bool)
    IsServiceActive(label string) bool
    GetActiveServices() []string
}
```

**Design Principles:**
1. **Simplicity**: The Service Manager focuses only on basic start/stop operations
2. **No Dependency Logic**: It doesn't understand or manage dependencies between services
3. **State Tracking**: Maintains a simple map of active services and their stop channels
4. **Reporter Pattern**: Uses a reporter interface to notify about service state changes

### Orchestrator (`internal/orchestrator`)

The Orchestrator is the higher-level component that manages the overall application state, including dependency management, health monitoring, and coordinated service lifecycle operations.

**Key Responsibilities:**
- Building and managing the dependency graph
- Monitoring Kubernetes connection health
- Managing cascade stops when dependencies fail
- Ensuring services start in the correct dependency order
- Handling service restarts with dependency awareness
- Tracking why services were stopped (manual vs dependency failure)

**Core Features:**

#### 1. Dependency Management
The Orchestrator builds a dependency graph (`internal/dependency`) that models relationships between:
- Kubernetes connections (MC and WC)
- Port forwards (which depend on K8s connections)
- MCP servers (which may depend on port forwards)

Example dependency chain:
```
K8s MC Connection → prometheus-mc (port forward) → prometheus (MCP)
K8s WC Connection → grafana-wc (port forward) → grafana (MCP)
```

#### 2. Health Monitoring
The Orchestrator continuously monitors Kubernetes connection health:
- Performs periodic health checks (default: 15 seconds)
- Tracks state changes (healthy → unhealthy → healthy)
- Only acts on state changes, not every health check
- Reports health status through the reporter interface

#### 3. Cascade Stop Logic
When a dependency becomes unhealthy or is stopped:
1. The Orchestrator identifies all transitive dependents using the dependency graph
2. It stops dependent services in reverse dependency order
3. It tracks that these services were stopped due to dependency failure
4. Example: If K8s WC connection fails → stop dependent port forwards → stop MCPs depending on those port forwards

#### 4. Automatic Recovery
When a failed dependency recovers:
1. The Orchestrator identifies services that were stopped due to that dependency
2. It restarts only those services (not manually stopped ones)
3. Services are started in proper dependency order
4. The restart is triggered automatically without user intervention

#### 5. Stop Reason Tracking
The Orchestrator maintains a `stopReasons` map to differentiate between:
- `StopReasonManual`: User explicitly stopped the service
- `StopReasonDependency`: Service stopped due to dependency failure

This ensures that only dependency-stopped services are restarted during recovery.

### Integration and Workflow

#### Startup Flow:
1. **Orchestrator Initialization**: Creates dependency graph based on configuration
2. **Health Check**: Performs initial K8s health checks
3. **Service Start**: Starts only services whose dependencies are healthy
4. **Dependency Ordering**: Uses topological sort to start services in correct order
5. **Monitoring**: Begins periodic health monitoring

#### Health State Change Flow:
1. **Detection**: Health check detects K8s connection state change
2. **Analysis**: Orchestrator analyzes dependency graph for affected services
3. **Action**: 
   - If unhealthy: Stop dependent services with cascade logic
   - If recovered: Restart dependency-stopped services
4. **Reporting**: Updates reported through the reporter interface

#### Service Operations:
- **Start**: Orchestrator ensures dependencies are met before starting
- **Stop**: Orchestrator performs cascade stop for dependents
- **Restart**: Orchestrator marks for restart and manages the lifecycle

### Usage in TUI and Non-TUI Modes

Both TUI and non-TUI modes use the same Orchestrator/Service Manager architecture:

**TUI Mode (`internal/tui`):**
- The TUI controller creates and manages the Orchestrator
- Service operations (stop/restart) go through the Orchestrator
- State updates are received via the reporter interface and converted to TUI messages
- The TUI model tracks service states for display

**Non-TUI Mode (`cmd/connect.go`):**
- Creates Orchestrator with console reporter
- Starts services through the Orchestrator
- Handles graceful shutdown on interrupt
- Displays health and service status to console

### Testing Considerations

The architecture supports comprehensive testing through:
1. **Mock Interfaces**: Both Service Manager and Kube Manager can be mocked
2. **Dependency Injection**: Components are injected, making testing easier
3. **State Verification**: Tests can verify stop reasons and service states
4. **Timing Control**: Health check intervals can be adjusted for faster tests

Example test pattern:
```go
// Mock the Kubernetes manager to simulate health state changes
kubeMgr := &mockKubeManager{}
kubeMgr.On("GetClusterNodeHealth", ...).Return(unhealthy).Once()
kubeMgr.On("GetClusterNodeHealth", ...).Return(healthy).Maybe()

// Create orchestrator with mocks
orch := orchestrator.New(kubeMgr, serviceMgr, reporter, config)

// Verify cascade stop behavior
// ... test assertions ...
```

### Best Practices for Extending

When adding new features that involve service management:

1. **Use the Orchestrator**: Don't bypass it for service operations
2. **Update Dependencies**: If adding new service types, update the dependency graph builder
3. **Maintain Separation**: Keep basic operations in Service Manager, complex logic in Orchestrator
4. **Test Thoroughly**: Include tests for dependency scenarios and edge cases
5. **Consider Health**: If the service depends on external resources, integrate with health monitoring
