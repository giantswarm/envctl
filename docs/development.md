# Developing envctl

This document provides guidelines and information for developers contributing to the envctl project.

## Prerequisites

*   Go 1.21+
*   Make
*   Docker (for `act` testing)
*   [act](https://github.com/nektos/act#installation) (optional, for local workflow testing)
*   Python 3+ & pip (for `yamllint`)
*   `yamllint` (`pip install yamllint`)
*   `golangci-lint` (can be installed via the CI workflow or locally: [Install Instructions](https://golangci-lint.run/usage/install/#local-installation))

## Development Setup

1. Clone the repository:
   ```zsh
   git clone https://github.com/giantswarm/envctl.git
   cd envctl
   ```

2. Install Go dependencies:
   ```zsh
   go mod download
   ```

3. Build the binary:
   ```zsh
   make build
   ```

4. Install locally for testing:
   ```zsh
   make install
   ```

## Linting and Testing Strategy

*   **Go Linting:** Handled by CircleCI (not run in this repository's GitHub Actions workflows).
*   **YAML Linting:** Performed using `yamllint`. Run locally using `make lint-yaml` or `make check` (which is also run in the CI workflow).
*   **Go Unit Tests:** Not currently run via Make targets or the GitHub Actions workflows in this repository. They should be run manually (`go test ./...`) or handled by a separate CI/CD process if configured (e.g., CircleCI).
*   **Release Dry-Run:** The GoReleaser configuration and build process can be tested using `make release-dry-run`. This is also run automatically in the CI workflow for pull requests.

## Automated Release Process

The envctl project uses GitHub Actions to automatically create releases when pull requests are merged to the main branch. Here's how it works:

1. When a PR is merged to `main`, the `.github/workflows/auto-release.yaml` GitHub Action workflow is triggered.
2. The workflow:
   * Gets the latest version tag (defaulting to v0.0.0).
   * Creates a new version by incrementing the patch number (e.g., v0.1.0 -> v0.1.1).
   * Creates and pushes a new git tag corresponding to the new version.
   * Uses GoReleaser (`goreleaser release --clean`) to build binaries, create archives, generate checksums, and create a GitHub Release.
   * The release notes (changelog) are automatically generated by GoReleaser using GitHub's API based on merged PRs since the last tag (`changelog.use: github-native`).

## Testing the Release Process

You can test parts of the release process locally using the following commands:

```zsh
# Test the GoReleaser build/archive/checksum steps without publishing
make release-dry-run

# Create a full release locally (builds, archives, checksums, GitHub release)
# Requires a GITHUB_TOKEN environment variable with repo scope.
# Note: This creates a real release and should generally only be used by maintainers for specific testing.
make release-local
```

### Testing the GitHub Actions Workflows Locally with `act`

You can use [`act`](https://github.com/nektos/act#installation) to simulate the GitHub Actions workflows locally. This requires Docker. Use the following `make` targets:

1. **Test the CI Workflow (Pull Request Event):**
   ```zsh
   # This simulates the checks run on a PR (yamllint, goreleaser dry-run)
   make test-ci-pr
   ```

2. **Test the CI Workflow (Push Event):**
   ```zsh
   # This simulates the checks run on a push to main (yamllint)
   make test-ci-push
   ```

3. **Test the Auto-Release Workflow:**
   ```zsh
   # This simulates the full release process
   make test-auto-release
   ```
   * **Note:** Requires `merged_pr_event.json` in the project root. You can create one based on the [GitHub pull_request event docs](https://docs.github.com/en/actions/using-workflows/events-that-trigger-workflows#pull_request) (ensure `.pull_request.merged` is `true`).
   * **Note:** The step involving `git push` for the tag might fail locally if the tag already exists or due to credential issues. You also won't see the GitHub Release creation locally.

## Contributing

1. Create a new branch for your changes:
   ```zsh
   git checkout -b feature/your-feature-name
   ```

2. Make your changes and commit them with meaningful commit messages.

3. Ensure your changes pass local checks (e.g., `make check`). Run Go linters and unit tests separately if they are not covered by other CI.

4. Push your branch and create a pull request targeting the `main` branch.

5. The CI workflow (`ci.yaml`) will run YAML linting checks (`make check`) and a GoReleaser dry-run on your PR.

6. Ensure your PR includes any necessary updates, but **do not manually update `CHANGELOG.md`**. The changelog will be generated automatically during the release.

7. Once your PR is approved and merged, the auto-release workflow will run and create a new release with generated notes.

## Version Management

The version number follows Semantic Versioning (MAJOR.MINOR.PATCH).

* The automated release process currently **only increments the PATCH version** for each merged PR.
* For **MINOR** or **MAJOR** version bumps, manual intervention is currently required after merging the relevant feature/breaking change PRs:
  1. Create and push the desired tag manually (e.g., `git tag v1.0.0`, `git push origin v1.0.0`).
  2. Manually trigger the `auto-release.yaml` workflow via the GitHub Actions UI, selecting the `main` branch and providing the manually created tag.
  *Alternatively, adjust the `Determine Next Version` step in `auto-release.yaml` temporarily before merging the PR that should trigger the bump, or create a separate release PR.*

## TUI Implementation Details

For detailed information about the TUI (Terminal User Interface) architecture, components, and how it interacts with core logic, please refer to the [TUI Implementation Notes](./tui-implementation.md).

## Unified Service Architecture

envctl uses a unified service architecture to manage all types of services (K8s connections, port forwards, and MCP servers) consistently. This architecture is built on two main components:

- **Service Manager**: Handles basic lifecycle operations for all services
- **Orchestrator**: Manages dependencies, health monitoring, and coordinated operations

For a comprehensive understanding of how services are managed, including:
- How K8s connections, port forwards, and MCP servers are treated as services
- The dependency graph and cascade operations
- Health monitoring and automatic recovery
- State management and reporting

Please refer to the [Architecture Overview](architecture.md) and the sections below on the Orchestrator and Service Architecture.

## Dependency Graph Implementation

The dependency graph is implemented in `internal/dependency/graph.go` and provides a simple but effective way to model service relationships.

### Core Types

```go
// NodeID is the unique identifier for a node
type NodeID string

// Node represents a service with its dependencies
type Node struct {
    ID           NodeID
    FriendlyName string
    Kind         NodeKind
    DependsOn    []NodeID  // What this node depends on
    State        NodeState // Current lifecycle state
}

// Graph manages the collection of nodes
type Graph struct {
    nodes map[NodeID]*Node
}
```

### Key Methods

1. **`AddNode(n Node)`**: Adds or replaces a node in the graph
2. **`Get(id NodeID) *Node`**: Retrieves a node by ID
3. **`Dependencies(id NodeID) []NodeID`**: Returns what a node depends ON
4. **`Dependents(id NodeID) []NodeID`**: Returns what depends on this node

### Usage Example

```go
// Building the graph (in orchestrator.buildDependencyGraph)
g := dependency.New()

// Add K8s connection node
g.AddNode(dependency.Node{
    ID:           "k8s:my-cluster",
    FriendlyName: "K8s Connection",
    Kind:         dependency.KindK8sConnection,
    DependsOn:    nil, // No dependencies
})

// Add port forward that depends on K8s
g.AddNode(dependency.Node{
    ID:           "pf:prometheus",
    FriendlyName: "Prometheus Port Forward",
    Kind:         dependency.KindPortForward,
    DependsOn:    []dependency.NodeID{"k8s:my-cluster"},
})

// Add MCP that depends on port forward
g.AddNode(dependency.Node{
    ID:           "mcp:prometheus",
    FriendlyName: "Prometheus MCP Server",
    Kind:         dependency.KindMCP,
    DependsOn:    []dependency.NodeID{"pf:prometheus"},
})
```

### Finding Dependencies

The Orchestrator uses `findAllDependents` to find all transitive dependents:

```go
// findAllDependents finds all services that depend on the given node
func (o *Orchestrator) findAllDependents(nodeID string) []string {
    allDependents := make(map[string]bool)
    visited := make(map[string]bool)
    
    var findDependents func(currentID string)
    findDependents = func(currentID string) {
        if visited[currentID] {
            return
        }
        visited[currentID] = true
        
        // Get direct dependents
        directDependents := o.depGraph.Dependents(dependency.NodeID(currentID))
        
        for _, dep := range directDependents {
            depStr := string(dep)
            if !strings.HasPrefix(depStr, "k8s:") {
                allDependents[depStr] = true
            }
            // Recursively find dependents
            findDependents(depStr)
        }
    }
    
    findDependents(nodeID)
    return keys(allDependents)
}
```

This recursive approach ensures that stopping a K8s connection will cascade through all port forwards to all MCP servers that transitively depend on it.

## Service Health Monitoring

envctl includes comprehensive health monitoring for all managed services:

### Kubernetes Connection Health
- Monitors the health of Management Cluster (MC) and Workload Cluster (WC) connections
- Checks node readiness and API server availability every 15 seconds
- Automatically stops dependent services when a connection becomes unhealthy
- Restarts services when connections recover

### MCP Server Health Checks
- Verifies MCP servers are responsive by attempting to list their tools via JSON-RPC
- Uses the `tools/list` method over HTTP to validate the server is functioning
- Checks run every 30 seconds for all running MCP servers
- Health status is reflected in the TUI with appropriate indicators

### Port Forward Health Checks
- Validates port forwards by attempting TCP connections to the local port
- Ensures the tunnel is active and accepting connections
- Runs every 30 seconds for all active port forwards
- Failed health checks are reported but don't automatically stop the service

### Health Check Implementation
The health checking system is implemented in `internal/orchestrator/orchestrator_health.go` with:
- `ServiceHealthChecker` interface for extensibility
- `MCPHealthChecker` for MCP server validation
- `PortForwardHealthChecker` for port forward validation
- Automatic health monitoring started by the orchestrator

### State Reconciliation
The TUI includes a `ReconcileState()` method that:
- Ensures the TUI model reflects the true state from the StateStore
- Runs on startup and can be triggered manually
- Prevents state drift between the TUI and the underlying services

## Dependency Management

## Recent Architectural Improvements

### Enhanced State Management (Issue #46)
The state management system has been significantly improved to address issues with state duplication and message ordering:

**Phase 1: Unified State Management**
- Added helper methods to TUI Model to use StateStore as single source of truth
- Implemented `GetServiceState()`, `GetServiceSnapshot()`, `ReconcileState()` methods
- TUI controller now queries StateStore instead of maintaining separate state

**Phase 2: Message Sequencing**
- Added sequence numbers to `ManagedServiceUpdate` messages
- Implemented `MessageBuffer` for handling out-of-order messages
- Ensures state updates are applied in the correct order

**Phase 3: Enhanced Correlation Tracking**
- Added `CascadeInfo` type for tracking cascade relationships
- Added `StateTransition` type for tracking state changes
- StateStore automatically records transitions and cascades
- Correlation IDs link related operations across services

**Phase 4: Improved Error Handling**
- Added retry logic for dropped critical updates
- Implemented backpressure notifications for users
- Enhanced TUIReporter with metrics and retry handling

### Dependency-Aware Service Restart
The orchestrator now properly handles service restarts with dependency awareness:

**When Restarting a Service:**
1. The orchestrator checks if all dependencies are running
2. If any dependency is stopped, it's included in the restart operation
3. All services are started in proper dependency order
4. This ensures services always have their requirements satisfied

**Example:**
Restarting the "grafana" MCP server will automatically restart the "mc-grafana" port forward if it's not running.

### Stop Reason Tracking
Services now track why they were stopped:

**Stop Reasons:**
- `StopReasonManual`: User explicitly stopped the service
- `StopReasonDependency`: Service stopped due to dependency failure

**Behavior:**
- Manually stopped services won't auto-restart
- Dependency-stopped services auto-restart when dependencies recover
- The monitoring goroutine respects these stop reasons

### Improved Restart Delay
When restarting services (especially port forwards), a 1-second delay is added to ensure:
- Ports are properly released by the OS
- Prevents "address already in use" errors
- Allows graceful cleanup of resources

## Testing Strategy

### Unit Tests
The codebase includes comprehensive unit tests for:
- Orchestrator dependency management
- Service lifecycle operations
- State management
- Health checking logic

### Mock Interfaces
Key interfaces are mocked for testing:
- `Service` interface for creating mock services
- `ServiceRegistry` for service registration and discovery
- `KubeManager` for Kubernetes operations
- `HealthChecker` for service health checks

### Test Patterns
Common test patterns include:
- Simulating health state changes
- Verifying cascade stop behavior
- Testing restart with dependencies
- Checking stop reason tracking

Example test pattern:
```go
// Configure test scenario
cfg := Config{
    MCName: "test-mc",
    WCName: "test-wc",
    PortForwards: []config.PortForwardDefinition{...},
    MCPServers: []config.MCPServerDefinition{...},
}

// Create orchestrator
orch := New(cfg)

// Start orchestrator
ctx := context.Background()
err := orch.Start(ctx)

// Get a service and check its state
service, exists := orch.GetServiceRegistry().Get("k8s-mc-test-mc")
assert.True(t, exists)
assert.Equal(t, services.StateRunning, service.GetState())

// Verify cascade stop behavior
err = orch.StopService("k8s-mc-test-mc")
// Check that dependent services were stopped
// ... test assertions ...
```

## Code Organization

### Package Structure
```
internal/
├── orchestrator/         # High-level coordination and dependency management
│   ├── orchestrator.go   # Main orchestrator logic
│   ├── orchestrator_deps.go  # Dependency management functions
│   ├── orchestrator_health.go # Health monitoring functions
│   └── orchestrator_test.go   # Comprehensive tests
├── services/            # Service abstraction and registry
│   ├── interface.go     # Service interface definitions
│   ├── registry.go      # Service registry implementation
│   ├── k8s/            # K8s connection service
│   ├── portforward/    # Port forward service
│   └── mcpserver/      # MCP server service
├── dependency/          # Dependency graph implementation
│   └── graph.go        # Graph data structure and algorithms
├── kube/               # Kubernetes cluster management
│   └── manager.go      # Kubernetes operations
├── api/                # API layer for service management
│   └── interfaces.go   # API interface definitions
├── config/             # Configuration management
│   └── types.go        # Configuration structures
└── tui/                # Terminal UI implementation
    ├── model/          # TUI state and business logic
    ├── view/           # UI rendering
    └── controller/     # User input handling
```

### Key Interfaces
1. **ServiceRegistry**: Service registration and discovery
2. **Service**: Common interface for all service types
3. **OrchestratorAPI**: High-level service orchestration
4. **HealthChecker**: Service health checking interface

## Future Improvements

### Planned Enhancements
1. **Persistent State**: Save and restore service state between sessions
2. **Custom Health Checks**: Allow user-defined health check scripts
3. **Service Groups**: Start/stop related services as a unit
4. **Enhanced Recovery**: Exponential backoff and circuit breakers

### Technical Debt
1. **Configuration Validation**: Stronger validation of service configurations
2. **Error Aggregation**: Better collection and presentation of multi-service errors
3. **Performance Monitoring**: Metrics for service startup times and resource usage

## Orchestrator and Service Architecture

The envctl application uses a sophisticated architecture to manage service dependencies, health monitoring, and lifecycle management. This is accomplished through the **Orchestrator** working with a **Service Registry** pattern.

### Service Registry (`internal/services`)

The Service Registry provides a centralized repository for all services in the system. It uses a flexible and extensible architecture where all services implement a common interface.

**Key Components:**
- **ServiceRegistry**: Interface for registering and retrieving services
- **Service**: Common interface implemented by all service types
- **ServiceType**: Enumeration of service types (K8s, PortForward, MCPServer)

**Core Interface (`ServiceRegistry`):**
```go
type ServiceRegistry interface {
    Register(service Service) error
    Unregister(label string) error
    Get(label string) (Service, bool)
    GetAll() []Service
    GetByType(serviceType ServiceType) []Service
}
```

**Service Interface:**
```go
type Service interface {
    GetLabel() string
    GetType() ServiceType
    GetState() ServiceState
    GetHealth() ServiceHealth
    Start(ctx context.Context) error
    Stop(ctx context.Context) error
    Restart(ctx context.Context) error
}
```

**Design Principles:**
1. **Unified Interface**: All services implement the same interface regardless of type
2. **Self-Contained**: Each service manages its own lifecycle and state
3. **Registry Pattern**: Services are registered centrally for easy discovery
4. **Type Safety**: Strong typing ensures compile-time checks
5. **Extensibility**: New service types can be added by implementing the interface

### K8s Connections as Services
Previously, K8s connections were managed separately from other services. This has been refactored to treat K8s connections as first-class services:

**Benefits:**
- Unified service management across all service types
- Consistent health monitoring and state reporting
- Better integration with the dependency system
- Simplified orchestrator logic

**Implementation:**
- K8s connections are now managed by `K8sConnectionService` in `internal/services/k8s/`
- They implement the common `Service` interface like all other services
- Health checks are integrated into the service lifecycle
- State changes are tracked consistently across all service types

### Health Check Implementation
The health checking system is implemented through:
- Health monitoring functions in `internal/orchestrator/orchestrator_health.go`
- Services that implement the `HealthChecker` interface can provide their own health checks
- The orchestrator automatically starts health check goroutines for services that support it
- Health checks run at intervals specified by each service (default 30 seconds)

# envctl Development Guide

This guide explains how to set up and use the systemd user service for envctl development, making debugging and testing much easier.

## Quick Setup

1. **First-time setup:**
   ```bash
   ./scripts/setup-systemd.sh
   ```

2. **Development workflow:**
   ```bash
   # Make your changes to the code
   ./scripts/dev-restart.sh
   ```

## systemd Service Benefits

- **Automatic restarts**: Service restarts automatically if it crashes
- **Easy management**: Use standard systemd commands
- **Centralized logging**: All logs go to journald
- **Background operation**: Runs in background, freeing up your terminal
- **Quick iteration**: `dev-restart.sh` rebuilds and restarts in seconds

## Commands

### Logging
```bash
# Follow logs in real-time
journalctl --user -u envctl.service -f --no-pager

# Show recent logs
journalctl --user -u envctl.service -n 50 --no-pager

# Show logs since a specific time
journalctl --user -u envctl.service --since "1 hour ago" --no-pager

# Show logs with timestamps
journalctl --user -u envctl.service -o short-precise --no-pager
```

### Development Workflow
```bash
# Quick development cycle
./scripts/dev-restart.sh

# Or manually:
go install .                                        # Install to ~/.go/bin
systemctl --user restart envctl.service --no-pager  # Restart service
journalctl --user -u envctl.service -f --no-pager   # Follow logs
```

## Debugging ServiceClasses

With the systemd service running, you can easily test ServiceClass functionality:

```bash
# Check if envctl is running
systemctl --user status envctl.service

# Test ServiceClass availability
# (using mcp-debug or Cursor MCP tools)
core_serviceclass_list
core_serviceclass_available --name=portforward

# Create a ServiceClass instance
core_service_create \
  --serviceClassName=portforward \
  --label=test-pf \
  --parameters='{"namespace":"mimir","resourceType":"service","resourceName":"mimir-query-frontend","localPort":8080,"targetPort":8080}'
```

## Troubleshooting

### Service won't start
```bash
# Check service status for errors
systemctl --user status envctl.service --no-pager

# Check recent logs
journalctl --user -u envctl.service -n 20 --no-pager
```

### Service keeps restarting
```bash
# Check logs for crash details
journalctl --user -u envctl.service --no-pager

# Look for specific errors
journalctl --user -u envctl.service --no-pager | grep -i error
```

## Integration with Development Tools

### VS Code / Cursor
- The service runs in the background, so your editor terminal is free
- Use mcp-debug tools to test functionality
- Logs are available via `journalctl` commands

### Git Workflow
```bash
# After making changes and committing
git add .
git commit -m "feat: implement new feature"
./scripts/dev-restart.sh  # Test the changes immediately
```

### Testing ServiceClass Changes
```bash
# Edit ServiceClass definitions
vim ~/.config/envctl/serviceclass/definitions/portforward.yaml

# Restart to reload definitions
./scripts/dev-restart.sh

# Test the updated ServiceClass
core_serviceclass_refresh
core_serviceclass_available --name=portforward
```

This setup significantly improves the development experience by providing a stable, manageable service that can be quickly restarted during development iterations.
